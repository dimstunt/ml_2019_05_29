{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Performance\n",
    "\n",
    "## Данные\n",
    "Рассмотрим набор данных [Student Performance Data Set](https://archive.ics.uci.edu/ml/datasets/student+performance): данные об успеваемости учащихся.\n",
    "\n",
    "Имеется 2 набора данных: математика (395 студентов) и португальский язык (649 студентов), но первый почти полностью лежит во втором, поэтому будем рассматривать только данные об изучающих португальский язык.\n",
    "\n",
    "Для каждого студента имеется 30 признаков + 3 целевых признака (G1, G2, G3) - оценка по курсу (за первую часть курса, за вторую часть курса и итоговая).\n",
    "\n",
    "## Задание\n",
    "Нужно решить задачу предсказания целевого признака G3 итоговой оценки студента по курсу. G3 сильно коррелирует с G1 и G2, поэтому пользоваться этими признаками нельзя. Для оценки качества моделей использовать кросс-валидацию с 10 фолдами, функция потерь - Mean Absolute Error $L(y_{true}, y_{pred}) = \\frac{1}{l}\\sum_{i=1}^l |y_{true, i} - y_{pred, i}|$. Ваша задача - получить наименьшую ошибку на кросс-валидации, для этого поэкспериментируйте с разными моделями машинного обучения и разными признаковыми описаними объектов. Результаты оформите в виде Jupyter Notebook. Весь код должен без проблем запускаться и отрабатывать. Также оформите некоторые выводы (какие методы Вы использовали, что получилось, какой метод лучший).\n",
    "\n",
    "Замечения:\n",
    "1. Для загрузки данных можно использовать pandas, для машинного обучения - sklearn.\n",
    "2. Для кросс-валидации можно использовать модуль sklearn.model_selection. Для воспроизводимости результатов явно инициализируйте генератор случайных чисел (разберитесь, как).\n",
    "3. Обратите внимание, что многие признаки в наборе данных - не вещественные, а категориальные (categorical) или порядковые (ordinal). О том, как работать с такими признаками, можно начать читать [здесь](https://dyakonov.org/2016/08/03/python-%d0%ba%d0%b0%d1%82%d0%b5%d0%b3%d0%be%d1%80%d0%b8%d0%b0%d0%bb%d1%8c%d0%bd%d1%8b%d0%b5-%d0%bf%d1%80%d0%b8%d0%b7%d0%bd%d0%b0%d0%ba%d0%b8/).\n",
    "4. Обратите внимание, что целевой признак - целые числа от 0 до 20.\n",
    "5. Обратите внимание, что не все алгоритмы обучения способны работать с L1-loss. Но не факт, что их нужно обучать с L1-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-8603bfc5d13b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylabtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import Ridge, Lasso, SGDRegressor, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.core.pylabtools import figsize\n",
    "from sklearn import tree\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('student-por.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['G1', 'G2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 649 entries, 0 to 648\n",
      "Data columns (total 31 columns):\n",
      "school        649 non-null object\n",
      "sex           649 non-null object\n",
      "age           649 non-null int64\n",
      "address       649 non-null object\n",
      "famsize       649 non-null object\n",
      "Pstatus       649 non-null object\n",
      "Medu          649 non-null int64\n",
      "Fedu          649 non-null int64\n",
      "Mjob          649 non-null object\n",
      "Fjob          649 non-null object\n",
      "reason        649 non-null object\n",
      "guardian      649 non-null object\n",
      "traveltime    649 non-null int64\n",
      "studytime     649 non-null int64\n",
      "failures      649 non-null int64\n",
      "schoolsup     649 non-null object\n",
      "famsup        649 non-null object\n",
      "paid          649 non-null object\n",
      "activities    649 non-null object\n",
      "nursery       649 non-null object\n",
      "higher        649 non-null object\n",
      "internet      649 non-null object\n",
      "romantic      649 non-null object\n",
      "famrel        649 non-null int64\n",
      "freetime      649 non-null int64\n",
      "goout         649 non-null int64\n",
      "Dalc          649 non-null int64\n",
      "Walc          649 non-null int64\n",
      "health        649 non-null int64\n",
      "absences      649 non-null int64\n",
      "G3            649 non-null int64\n",
      "dtypes: int64(14), object(17)\n",
      "memory usage: 157.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "      <td>649.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.744222</td>\n",
       "      <td>2.514638</td>\n",
       "      <td>2.306626</td>\n",
       "      <td>1.568567</td>\n",
       "      <td>1.930663</td>\n",
       "      <td>0.221880</td>\n",
       "      <td>3.930663</td>\n",
       "      <td>3.180277</td>\n",
       "      <td>3.184900</td>\n",
       "      <td>1.502311</td>\n",
       "      <td>2.280431</td>\n",
       "      <td>3.536210</td>\n",
       "      <td>3.659476</td>\n",
       "      <td>11.906009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.218138</td>\n",
       "      <td>1.134552</td>\n",
       "      <td>1.099931</td>\n",
       "      <td>0.748660</td>\n",
       "      <td>0.829510</td>\n",
       "      <td>0.593235</td>\n",
       "      <td>0.955717</td>\n",
       "      <td>1.051093</td>\n",
       "      <td>1.175766</td>\n",
       "      <td>0.924834</td>\n",
       "      <td>1.284380</td>\n",
       "      <td>1.446259</td>\n",
       "      <td>4.640759</td>\n",
       "      <td>3.230656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age        Medu        Fedu  traveltime   studytime    failures  \\\n",
       "count  649.000000  649.000000  649.000000  649.000000  649.000000  649.000000   \n",
       "mean    16.744222    2.514638    2.306626    1.568567    1.930663    0.221880   \n",
       "std      1.218138    1.134552    1.099931    0.748660    0.829510    0.593235   \n",
       "min     15.000000    0.000000    0.000000    1.000000    1.000000    0.000000   \n",
       "25%     16.000000    2.000000    1.000000    1.000000    1.000000    0.000000   \n",
       "50%     17.000000    2.000000    2.000000    1.000000    2.000000    0.000000   \n",
       "75%     18.000000    4.000000    3.000000    2.000000    2.000000    0.000000   \n",
       "max     22.000000    4.000000    4.000000    4.000000    4.000000    3.000000   \n",
       "\n",
       "           famrel    freetime       goout        Dalc        Walc      health  \\\n",
       "count  649.000000  649.000000  649.000000  649.000000  649.000000  649.000000   \n",
       "mean     3.930663    3.180277    3.184900    1.502311    2.280431    3.536210   \n",
       "std      0.955717    1.051093    1.175766    0.924834    1.284380    1.446259   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      4.000000    3.000000    2.000000    1.000000    1.000000    2.000000   \n",
       "50%      4.000000    3.000000    3.000000    1.000000    2.000000    4.000000   \n",
       "75%      5.000000    4.000000    4.000000    2.000000    3.000000    5.000000   \n",
       "max      5.000000    5.000000    5.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "         absences          G3  \n",
       "count  649.000000  649.000000  \n",
       "mean     3.659476   11.906009  \n",
       "std      4.640759    3.230656  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000   10.000000  \n",
       "50%      2.000000   12.000000  \n",
       "75%      6.000000   14.000000  \n",
       "max     32.000000   19.000000  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 31 columns.\n",
      "There are 0 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Values, % of Total Values]\n",
       "Index: []"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "figsize(8, 8)\n",
    "# Histogram of the Energy Star Score\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.hist(df['G3'].dropna(), bins = 21, edgecolor = 'k');\n",
    "plt.xlabel('G3'); plt.ylabel('Number of Buildings');\n",
    "plt.title('G3 Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failures     -0.393316\n",
      "Dalc         -0.204719\n",
      "Walc         -0.176619\n",
      "traveltime   -0.127173\n",
      "freetime     -0.122705\n",
      "age          -0.106505\n",
      "health       -0.098851\n",
      "absences     -0.091379\n",
      "goout        -0.087641\n",
      "famrel        0.063361\n",
      "Fedu          0.211800\n",
      "Medu          0.240151\n",
      "studytime     0.249789\n",
      "G3            1.000000\n",
      "Name: G3, dtype: float64 \n",
      "\n",
      "failures     -0.393316\n",
      "Dalc         -0.204719\n",
      "Walc         -0.176619\n",
      "traveltime   -0.127173\n",
      "freetime     -0.122705\n",
      "age          -0.106505\n",
      "health       -0.098851\n",
      "absences     -0.091379\n",
      "goout        -0.087641\n",
      "famrel        0.063361\n",
      "Fedu          0.211800\n",
      "Medu          0.240151\n",
      "studytime     0.249789\n",
      "G3            1.000000\n",
      "Name: G3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlations_df = df.corr()['G3'].sort_values()\n",
    "print(correlations_df.head(15), '\\n')\n",
    "print(correlations_df.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model\n",
    "        to generalize and improves the interpretability of the model.\n",
    "        \n",
    "    Inputs: \n",
    "        threshold: any features with correlations greater than this value are removed\n",
    "    \n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "    \n",
    "    # Dont want to remove correlations between Energy Star Score\n",
    "    y = x['G3']\n",
    "    x = x.drop(columns = ['G3'])\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "            \n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns = drops)\n",
    "    \n",
    "    # Add the score back in to the data\n",
    "    x['G3'] = y\n",
    "               \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(649, 74)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.copy()\n",
    "numeric_subset = df.select_dtypes('number')\n",
    "for c in numeric_subset.columns:\n",
    "    if c == 'G3':\n",
    "        next\n",
    "    else:\n",
    "        numeric_subset['log_' + c] = np.log(numeric_subset[c])\n",
    "categorical_subset = df[['Medu', 'Fedu', 'traveltime', 'studytime', 'school', 'sex',\\\n",
    "                            'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', \\\n",
    "                            'guardian', 'schoolsup', 'famsup', 'paid', 'activities', \\\n",
    "                            'nursery', 'higher', 'internet', 'romantic']]\n",
    "categorical_subset = pd.get_dummies(categorical_subset)\n",
    "features = pd.concat([numeric_subset, categorical_subset], axis = 1)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 649 entries, 0 to 648\n",
      "Data columns (total 74 columns):\n",
      "age                  649 non-null int64\n",
      "Medu                 649 non-null int64\n",
      "Fedu                 649 non-null int64\n",
      "traveltime           649 non-null int64\n",
      "studytime            649 non-null int64\n",
      "failures             649 non-null int64\n",
      "famrel               649 non-null int64\n",
      "freetime             649 non-null int64\n",
      "goout                649 non-null int64\n",
      "Dalc                 649 non-null int64\n",
      "Walc                 649 non-null int64\n",
      "health               649 non-null int64\n",
      "absences             649 non-null int64\n",
      "G3                   649 non-null int64\n",
      "log_age              649 non-null float64\n",
      "log_Medu             649 non-null float64\n",
      "log_Fedu             649 non-null float64\n",
      "log_traveltime       649 non-null float64\n",
      "log_studytime        649 non-null float64\n",
      "log_failures         649 non-null float64\n",
      "log_famrel           649 non-null float64\n",
      "log_freetime         649 non-null float64\n",
      "log_goout            649 non-null float64\n",
      "log_Dalc             649 non-null float64\n",
      "log_Walc             649 non-null float64\n",
      "log_health           649 non-null float64\n",
      "log_absences         649 non-null float64\n",
      "Medu                 649 non-null int64\n",
      "Fedu                 649 non-null int64\n",
      "traveltime           649 non-null int64\n",
      "studytime            649 non-null int64\n",
      "school_GP            649 non-null uint8\n",
      "school_MS            649 non-null uint8\n",
      "sex_F                649 non-null uint8\n",
      "sex_M                649 non-null uint8\n",
      "address_R            649 non-null uint8\n",
      "address_U            649 non-null uint8\n",
      "famsize_GT3          649 non-null uint8\n",
      "famsize_LE3          649 non-null uint8\n",
      "Pstatus_A            649 non-null uint8\n",
      "Pstatus_T            649 non-null uint8\n",
      "Mjob_at_home         649 non-null uint8\n",
      "Mjob_health          649 non-null uint8\n",
      "Mjob_other           649 non-null uint8\n",
      "Mjob_services        649 non-null uint8\n",
      "Mjob_teacher         649 non-null uint8\n",
      "Fjob_at_home         649 non-null uint8\n",
      "Fjob_health          649 non-null uint8\n",
      "Fjob_other           649 non-null uint8\n",
      "Fjob_services        649 non-null uint8\n",
      "Fjob_teacher         649 non-null uint8\n",
      "reason_course        649 non-null uint8\n",
      "reason_home          649 non-null uint8\n",
      "reason_other         649 non-null uint8\n",
      "reason_reputation    649 non-null uint8\n",
      "guardian_father      649 non-null uint8\n",
      "guardian_mother      649 non-null uint8\n",
      "guardian_other       649 non-null uint8\n",
      "schoolsup_no         649 non-null uint8\n",
      "schoolsup_yes        649 non-null uint8\n",
      "famsup_no            649 non-null uint8\n",
      "famsup_yes           649 non-null uint8\n",
      "paid_no              649 non-null uint8\n",
      "paid_yes             649 non-null uint8\n",
      "activities_no        649 non-null uint8\n",
      "activities_yes       649 non-null uint8\n",
      "nursery_no           649 non-null uint8\n",
      "nursery_yes          649 non-null uint8\n",
      "higher_no            649 non-null uint8\n",
      "higher_yes           649 non-null uint8\n",
      "internet_no          649 non-null uint8\n",
      "internet_yes         649 non-null uint8\n",
      "romantic_no          649 non-null uint8\n",
      "romantic_yes         649 non-null uint8\n",
      "dtypes: float64(13), int64(18), uint8(43)\n",
      "memory usage: 184.5 KB\n"
     ]
    }
   ],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = remove_collinear_features(features, 0.6);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 649 entries, 0 to 648\n",
      "Data columns (total 53 columns):\n",
      "age                  649 non-null int64\n",
      "failures             649 non-null int64\n",
      "famrel               649 non-null int64\n",
      "freetime             649 non-null int64\n",
      "goout                649 non-null int64\n",
      "Dalc                 649 non-null int64\n",
      "Walc                 649 non-null int64\n",
      "health               649 non-null int64\n",
      "absences             649 non-null int64\n",
      "school_GP            649 non-null uint8\n",
      "school_MS            649 non-null uint8\n",
      "sex_F                649 non-null uint8\n",
      "sex_M                649 non-null uint8\n",
      "address_R            649 non-null uint8\n",
      "address_U            649 non-null uint8\n",
      "famsize_GT3          649 non-null uint8\n",
      "famsize_LE3          649 non-null uint8\n",
      "Pstatus_A            649 non-null uint8\n",
      "Pstatus_T            649 non-null uint8\n",
      "Mjob_at_home         649 non-null uint8\n",
      "Mjob_health          649 non-null uint8\n",
      "Mjob_other           649 non-null uint8\n",
      "Mjob_services        649 non-null uint8\n",
      "Mjob_teacher         649 non-null uint8\n",
      "Fjob_at_home         649 non-null uint8\n",
      "Fjob_health          649 non-null uint8\n",
      "Fjob_other           649 non-null uint8\n",
      "Fjob_services        649 non-null uint8\n",
      "Fjob_teacher         649 non-null uint8\n",
      "reason_course        649 non-null uint8\n",
      "reason_home          649 non-null uint8\n",
      "reason_other         649 non-null uint8\n",
      "reason_reputation    649 non-null uint8\n",
      "guardian_father      649 non-null uint8\n",
      "guardian_mother      649 non-null uint8\n",
      "guardian_other       649 non-null uint8\n",
      "schoolsup_no         649 non-null uint8\n",
      "schoolsup_yes        649 non-null uint8\n",
      "famsup_no            649 non-null uint8\n",
      "famsup_yes           649 non-null uint8\n",
      "paid_no              649 non-null uint8\n",
      "paid_yes             649 non-null uint8\n",
      "activities_no        649 non-null uint8\n",
      "activities_yes       649 non-null uint8\n",
      "nursery_no           649 non-null uint8\n",
      "nursery_yes          649 non-null uint8\n",
      "higher_no            649 non-null uint8\n",
      "higher_yes           649 non-null uint8\n",
      "internet_no          649 non-null uint8\n",
      "internet_yes         649 non-null uint8\n",
      "romantic_no          649 non-null uint8\n",
      "romantic_yes         649 non-null uint8\n",
      "G3                   649 non-null int64\n",
      "dtypes: int64(10), uint8(43)\n",
      "memory usage: 78.0 KB\n"
     ]
    }
   ],
   "source": [
    "features  = features.dropna(axis=1, how = 'all')\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failures        -0.393316\n",
      "higher_no       -0.332172\n",
      "school_MS       -0.284294\n",
      "Dalc            -0.204719\n",
      "Walc            -0.176619\n",
      "address_R       -0.167637\n",
      "internet_no     -0.150025\n",
      "Mjob_at_home    -0.136778\n",
      "reason_other    -0.132577\n",
      "sex_M           -0.129077\n",
      "freetime        -0.122705\n",
      "age             -0.106505\n",
      "health          -0.098851\n",
      "reason_course   -0.098305\n",
      "absences        -0.091379\n",
      "Name: G3, dtype: float64 \n",
      "\n",
      "famsup_yes           0.059206\n",
      "activities_yes       0.059791\n",
      "famrel               0.063361\n",
      "schoolsup_no         0.066405\n",
      "romantic_no          0.090583\n",
      "Mjob_health          0.101244\n",
      "Fjob_teacher         0.125916\n",
      "sex_F                0.129077\n",
      "Mjob_teacher         0.134910\n",
      "internet_yes         0.150025\n",
      "address_U            0.167637\n",
      "reason_reputation    0.170944\n",
      "school_GP            0.284294\n",
      "higher_yes           0.332172\n",
      "G3                   1.000000\n",
      "Name: G3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlations_features = features.corr()['G3'].sort_values()\n",
    "print(correlations_features.head(15), '\\n')\n",
    "print(correlations_features.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 649 entries, 0 to 648\n",
      "Data columns (total 53 columns):\n",
      "age                  649 non-null int64\n",
      "failures             649 non-null int64\n",
      "famrel               649 non-null int64\n",
      "freetime             649 non-null int64\n",
      "goout                649 non-null int64\n",
      "Dalc                 649 non-null int64\n",
      "Walc                 649 non-null int64\n",
      "health               649 non-null int64\n",
      "absences             649 non-null int64\n",
      "school_GP            649 non-null uint8\n",
      "school_MS            649 non-null uint8\n",
      "sex_F                649 non-null uint8\n",
      "sex_M                649 non-null uint8\n",
      "address_R            649 non-null uint8\n",
      "address_U            649 non-null uint8\n",
      "famsize_GT3          649 non-null uint8\n",
      "famsize_LE3          649 non-null uint8\n",
      "Pstatus_A            649 non-null uint8\n",
      "Pstatus_T            649 non-null uint8\n",
      "Mjob_at_home         649 non-null uint8\n",
      "Mjob_health          649 non-null uint8\n",
      "Mjob_other           649 non-null uint8\n",
      "Mjob_services        649 non-null uint8\n",
      "Mjob_teacher         649 non-null uint8\n",
      "Fjob_at_home         649 non-null uint8\n",
      "Fjob_health          649 non-null uint8\n",
      "Fjob_other           649 non-null uint8\n",
      "Fjob_services        649 non-null uint8\n",
      "Fjob_teacher         649 non-null uint8\n",
      "reason_course        649 non-null uint8\n",
      "reason_home          649 non-null uint8\n",
      "reason_other         649 non-null uint8\n",
      "reason_reputation    649 non-null uint8\n",
      "guardian_father      649 non-null uint8\n",
      "guardian_mother      649 non-null uint8\n",
      "guardian_other       649 non-null uint8\n",
      "schoolsup_no         649 non-null uint8\n",
      "schoolsup_yes        649 non-null uint8\n",
      "famsup_no            649 non-null uint8\n",
      "famsup_yes           649 non-null uint8\n",
      "paid_no              649 non-null uint8\n",
      "paid_yes             649 non-null uint8\n",
      "activities_no        649 non-null uint8\n",
      "activities_yes       649 non-null uint8\n",
      "nursery_no           649 non-null uint8\n",
      "nursery_yes          649 non-null uint8\n",
      "higher_no            649 non-null uint8\n",
      "higher_yes           649 non-null uint8\n",
      "internet_no          649 non-null uint8\n",
      "internet_yes         649 non-null uint8\n",
      "romantic_no          649 non-null uint8\n",
      "romantic_yes         649 non-null uint8\n",
      "G3                   649 non-null int64\n",
      "dtypes: int64(10), uint8(43)\n",
      "memory usage: 78.0 KB\n"
     ]
    }
   ],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat = ['Medu', 'Fedu', 'traveltime', 'studytime', 'school', 'sex',\\\n",
    "                            'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', \\\n",
    "                            'guardian', 'schoolsup', 'famsup', 'paid', 'activities', \\\n",
    "                            'nursery', 'higher', 'internet', 'romantic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.drop(['G3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = features['G3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [[Ridge(random_state=123890), 0],\n",
    "          [SGDRegressor(loss='huber', random_state=123890), 0],\n",
    "          [Lasso(random_state=123890), 0],\n",
    "          [ElasticNet(random_state=123890), 0],\n",
    "          [tree.DecisionTreeRegressor(random_state=123890, criterion='mae'), 1],\n",
    "          [SVR(), 2],\n",
    "          [KNeighborsRegressor(metric='manhattan'), 3]]\n",
    "\n",
    "scorer = make_scorer(mean_absolute_error)\n",
    "res = {model[0].__class__.__name__: [] for model in models}\n",
    "p_g = [{'alpha': np.arange(0.001, 1.001, 0.001)},\n",
    "       {'max_depth':np.arange(1, 6)},\n",
    "       {'kernel': ['rbf', 'linear', 'poly'], 'C': np.arange(1, 100)},\n",
    "       {'n_neighbors':np.arange(1, 10)}\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2480 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done 6480 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:   36.9s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': [2.0480740836447655, {'alpha': 0.001}]}\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 620 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 3320 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 7820 tasks      | elapsed:   32.1s\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:   42.0s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': [2.838703648230505, {'alpha': 1.0}]}\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 620 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3320 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 7820 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:   33.7s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': [2.4410837209323923, {'alpha': 1.0}]}\n",
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 348 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1848 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 4348 tasks      | elapsed:   19.3s\n",
      "[Parallel(n_jobs=-1)]: Done 7848 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:   41.8s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': [2.3677918152769357, {'alpha': 1.0}]}\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    1.8s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': [2.2704160246533127, {'max_depth': 1}]}\n",
      "Fitting 10 folds for each of 297 candidates, totalling 2970 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 246 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 846 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1296 tasks      | elapsed: 30.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1846 tasks      | elapsed: 55.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed: 91.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2970 out of 2970 | elapsed: 120.3min finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': [3.927385806491669, {'C': 99, 'kernel': 'poly'}]}\n",
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:    1.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': [2.987673343605547, {'n_neighbors': 1}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  83 out of  90 | elapsed:    1.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    1.9s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "res = {model.__class__.__name__: 0 for model in models}\n",
    "for model in models:\n",
    "    gr = GridSearchCV(estimator=model[0], param_grid=p_g[model[1]],\n",
    "                  n_jobs=-1, verbose=1, cv=10, scoring=scorer, return_train_score=True\n",
    "             )\n",
    "    gr.fit(X, Y)\n",
    "    res[model.__class__.__name__] = [gr.best_score_, gr.best_params_]\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CatBoostRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-4362d8b40a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MAE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'CatBoostRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "lr = CatBoostRegressor(loss_function='MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = GridSearchCV(estimator=lr, param_grid={'alpha': np.arange(0.001, 1.001, 0.01)},\\\n",
    "                  n_jobs=-1, verbose=1, cv=10, scoring=scorer,\\\n",
    "                 return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 620 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    4.0s finished\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=123890, solver='auto', tol=0.001),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'alpha': array([0.001, 0.011, ..., 0.981, 0.991])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=make_scorer(mean_absolute_error), verbose=1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>1.435532</td>\n",
       "      <td>1.596441</td>\n",
       "      <td>1.833912</td>\n",
       "      <td>1.886301</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923662</td>\n",
       "      <td>1.893111</td>\n",
       "      <td>1.932538</td>\n",
       "      <td>1.884239</td>\n",
       "      <td>1.875665</td>\n",
       "      <td>1.890698</td>\n",
       "      <td>1.855118</td>\n",
       "      <td>1.687341</td>\n",
       "      <td>1.882834</td>\n",
       "      <td>0.071135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.011</td>\n",
       "      <td>{'alpha': 0.011}</td>\n",
       "      <td>1.435513</td>\n",
       "      <td>1.596463</td>\n",
       "      <td>1.833873</td>\n",
       "      <td>1.886279</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923662</td>\n",
       "      <td>1.893110</td>\n",
       "      <td>1.932532</td>\n",
       "      <td>1.884235</td>\n",
       "      <td>1.875662</td>\n",
       "      <td>1.890694</td>\n",
       "      <td>1.855118</td>\n",
       "      <td>1.687337</td>\n",
       "      <td>1.882832</td>\n",
       "      <td>0.071135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009925</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.021</td>\n",
       "      <td>{'alpha': 0.020999999999999998}</td>\n",
       "      <td>1.435493</td>\n",
       "      <td>1.596485</td>\n",
       "      <td>1.833834</td>\n",
       "      <td>1.886256</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923662</td>\n",
       "      <td>1.893109</td>\n",
       "      <td>1.932527</td>\n",
       "      <td>1.884232</td>\n",
       "      <td>1.875660</td>\n",
       "      <td>1.890691</td>\n",
       "      <td>1.855118</td>\n",
       "      <td>1.687333</td>\n",
       "      <td>1.882829</td>\n",
       "      <td>0.071135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.006721</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.031</td>\n",
       "      <td>{'alpha': 0.030999999999999996}</td>\n",
       "      <td>1.435474</td>\n",
       "      <td>1.596506</td>\n",
       "      <td>1.833795</td>\n",
       "      <td>1.886233</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923663</td>\n",
       "      <td>1.893108</td>\n",
       "      <td>1.932521</td>\n",
       "      <td>1.884228</td>\n",
       "      <td>1.875657</td>\n",
       "      <td>1.890687</td>\n",
       "      <td>1.855117</td>\n",
       "      <td>1.687329</td>\n",
       "      <td>1.882826</td>\n",
       "      <td>0.071136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006710</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.041</td>\n",
       "      <td>{'alpha': 0.040999999999999995}</td>\n",
       "      <td>1.435455</td>\n",
       "      <td>1.596527</td>\n",
       "      <td>1.833756</td>\n",
       "      <td>1.886210</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923663</td>\n",
       "      <td>1.893107</td>\n",
       "      <td>1.932516</td>\n",
       "      <td>1.884224</td>\n",
       "      <td>1.875654</td>\n",
       "      <td>1.890683</td>\n",
       "      <td>1.855117</td>\n",
       "      <td>1.687324</td>\n",
       "      <td>1.882824</td>\n",
       "      <td>0.071136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006998</td>\n",
       "      <td>0.004127</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.051</td>\n",
       "      <td>{'alpha': 0.05099999999999999}</td>\n",
       "      <td>1.435436</td>\n",
       "      <td>1.596549</td>\n",
       "      <td>1.833717</td>\n",
       "      <td>1.886187</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923664</td>\n",
       "      <td>1.893106</td>\n",
       "      <td>1.932511</td>\n",
       "      <td>1.884221</td>\n",
       "      <td>1.875651</td>\n",
       "      <td>1.890679</td>\n",
       "      <td>1.855117</td>\n",
       "      <td>1.687320</td>\n",
       "      <td>1.882821</td>\n",
       "      <td>0.071136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.009639</td>\n",
       "      <td>0.007851</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.061</td>\n",
       "      <td>{'alpha': 0.06099999999999999}</td>\n",
       "      <td>1.435417</td>\n",
       "      <td>1.596570</td>\n",
       "      <td>1.833678</td>\n",
       "      <td>1.886164</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923664</td>\n",
       "      <td>1.893106</td>\n",
       "      <td>1.932505</td>\n",
       "      <td>1.884217</td>\n",
       "      <td>1.875648</td>\n",
       "      <td>1.890675</td>\n",
       "      <td>1.855117</td>\n",
       "      <td>1.687316</td>\n",
       "      <td>1.882818</td>\n",
       "      <td>0.071137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007304</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.071</td>\n",
       "      <td>{'alpha': 0.071}</td>\n",
       "      <td>1.435398</td>\n",
       "      <td>1.596592</td>\n",
       "      <td>1.833639</td>\n",
       "      <td>1.886141</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923665</td>\n",
       "      <td>1.893105</td>\n",
       "      <td>1.932500</td>\n",
       "      <td>1.884213</td>\n",
       "      <td>1.875645</td>\n",
       "      <td>1.890671</td>\n",
       "      <td>1.855116</td>\n",
       "      <td>1.687311</td>\n",
       "      <td>1.882816</td>\n",
       "      <td>0.071137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.081</td>\n",
       "      <td>{'alpha': 0.08099999999999999}</td>\n",
       "      <td>1.435379</td>\n",
       "      <td>1.596613</td>\n",
       "      <td>1.833601</td>\n",
       "      <td>1.886118</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923665</td>\n",
       "      <td>1.893104</td>\n",
       "      <td>1.932495</td>\n",
       "      <td>1.884210</td>\n",
       "      <td>1.875642</td>\n",
       "      <td>1.890668</td>\n",
       "      <td>1.855116</td>\n",
       "      <td>1.687307</td>\n",
       "      <td>1.882813</td>\n",
       "      <td>0.071137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004861</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.091</td>\n",
       "      <td>{'alpha': 0.09099999999999998}</td>\n",
       "      <td>1.435360</td>\n",
       "      <td>1.596635</td>\n",
       "      <td>1.833562</td>\n",
       "      <td>1.886096</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923666</td>\n",
       "      <td>1.893103</td>\n",
       "      <td>1.932489</td>\n",
       "      <td>1.884206</td>\n",
       "      <td>1.875639</td>\n",
       "      <td>1.890664</td>\n",
       "      <td>1.855116</td>\n",
       "      <td>1.687303</td>\n",
       "      <td>1.882811</td>\n",
       "      <td>0.071138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.006266</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.101</td>\n",
       "      <td>{'alpha': 0.10099999999999998}</td>\n",
       "      <td>1.435341</td>\n",
       "      <td>1.596656</td>\n",
       "      <td>1.833523</td>\n",
       "      <td>1.886073</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923666</td>\n",
       "      <td>1.893102</td>\n",
       "      <td>1.932484</td>\n",
       "      <td>1.884202</td>\n",
       "      <td>1.875636</td>\n",
       "      <td>1.890660</td>\n",
       "      <td>1.855115</td>\n",
       "      <td>1.687299</td>\n",
       "      <td>1.882808</td>\n",
       "      <td>0.071138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.006812</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.111</td>\n",
       "      <td>{'alpha': 0.11099999999999999}</td>\n",
       "      <td>1.435322</td>\n",
       "      <td>1.596677</td>\n",
       "      <td>1.833484</td>\n",
       "      <td>1.886050</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923667</td>\n",
       "      <td>1.893101</td>\n",
       "      <td>1.932479</td>\n",
       "      <td>1.884199</td>\n",
       "      <td>1.875633</td>\n",
       "      <td>1.890656</td>\n",
       "      <td>1.855115</td>\n",
       "      <td>1.687294</td>\n",
       "      <td>1.882805</td>\n",
       "      <td>0.071138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.001827</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.121</td>\n",
       "      <td>{'alpha': 0.12099999999999998}</td>\n",
       "      <td>1.435303</td>\n",
       "      <td>1.596699</td>\n",
       "      <td>1.833445</td>\n",
       "      <td>1.886027</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923667</td>\n",
       "      <td>1.893100</td>\n",
       "      <td>1.932474</td>\n",
       "      <td>1.884195</td>\n",
       "      <td>1.875631</td>\n",
       "      <td>1.890652</td>\n",
       "      <td>1.855115</td>\n",
       "      <td>1.687290</td>\n",
       "      <td>1.882803</td>\n",
       "      <td>0.071139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.005957</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.131</td>\n",
       "      <td>{'alpha': 0.13099999999999998}</td>\n",
       "      <td>1.435284</td>\n",
       "      <td>1.596720</td>\n",
       "      <td>1.833406</td>\n",
       "      <td>1.886004</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923668</td>\n",
       "      <td>1.893100</td>\n",
       "      <td>1.932468</td>\n",
       "      <td>1.884191</td>\n",
       "      <td>1.875628</td>\n",
       "      <td>1.890648</td>\n",
       "      <td>1.855114</td>\n",
       "      <td>1.687286</td>\n",
       "      <td>1.882800</td>\n",
       "      <td>0.071139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.141</td>\n",
       "      <td>{'alpha': 0.141}</td>\n",
       "      <td>1.435265</td>\n",
       "      <td>1.596741</td>\n",
       "      <td>1.833367</td>\n",
       "      <td>1.885981</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923668</td>\n",
       "      <td>1.893099</td>\n",
       "      <td>1.932463</td>\n",
       "      <td>1.884188</td>\n",
       "      <td>1.875625</td>\n",
       "      <td>1.890645</td>\n",
       "      <td>1.855114</td>\n",
       "      <td>1.687282</td>\n",
       "      <td>1.882797</td>\n",
       "      <td>0.071139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.019860</td>\n",
       "      <td>0.021024</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.151</td>\n",
       "      <td>{'alpha': 0.15099999999999997}</td>\n",
       "      <td>1.435246</td>\n",
       "      <td>1.596763</td>\n",
       "      <td>1.833328</td>\n",
       "      <td>1.885958</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923668</td>\n",
       "      <td>1.893098</td>\n",
       "      <td>1.932458</td>\n",
       "      <td>1.884184</td>\n",
       "      <td>1.875622</td>\n",
       "      <td>1.890641</td>\n",
       "      <td>1.855114</td>\n",
       "      <td>1.687277</td>\n",
       "      <td>1.882795</td>\n",
       "      <td>0.071140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.011156</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.161</td>\n",
       "      <td>{'alpha': 0.16099999999999998}</td>\n",
       "      <td>1.435227</td>\n",
       "      <td>1.596784</td>\n",
       "      <td>1.833290</td>\n",
       "      <td>1.885936</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923669</td>\n",
       "      <td>1.893097</td>\n",
       "      <td>1.932453</td>\n",
       "      <td>1.884180</td>\n",
       "      <td>1.875619</td>\n",
       "      <td>1.890637</td>\n",
       "      <td>1.855113</td>\n",
       "      <td>1.687273</td>\n",
       "      <td>1.882792</td>\n",
       "      <td>0.071140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.017678</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>0.004713</td>\n",
       "      <td>0.005965</td>\n",
       "      <td>0.171</td>\n",
       "      <td>{'alpha': 0.17099999999999999}</td>\n",
       "      <td>1.435208</td>\n",
       "      <td>1.596805</td>\n",
       "      <td>1.833251</td>\n",
       "      <td>1.885913</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923669</td>\n",
       "      <td>1.893096</td>\n",
       "      <td>1.932448</td>\n",
       "      <td>1.884177</td>\n",
       "      <td>1.875616</td>\n",
       "      <td>1.890633</td>\n",
       "      <td>1.855113</td>\n",
       "      <td>1.687269</td>\n",
       "      <td>1.882790</td>\n",
       "      <td>0.071140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.012077</td>\n",
       "      <td>0.009928</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.181</td>\n",
       "      <td>{'alpha': 0.18099999999999997}</td>\n",
       "      <td>1.435189</td>\n",
       "      <td>1.596827</td>\n",
       "      <td>1.833212</td>\n",
       "      <td>1.885890</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923670</td>\n",
       "      <td>1.893095</td>\n",
       "      <td>1.932443</td>\n",
       "      <td>1.884173</td>\n",
       "      <td>1.875613</td>\n",
       "      <td>1.890629</td>\n",
       "      <td>1.855113</td>\n",
       "      <td>1.687265</td>\n",
       "      <td>1.882787</td>\n",
       "      <td>0.071141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.014294</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.191</td>\n",
       "      <td>{'alpha': 0.19099999999999998}</td>\n",
       "      <td>1.435170</td>\n",
       "      <td>1.596848</td>\n",
       "      <td>1.833173</td>\n",
       "      <td>1.885867</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923670</td>\n",
       "      <td>1.893094</td>\n",
       "      <td>1.932438</td>\n",
       "      <td>1.884169</td>\n",
       "      <td>1.875610</td>\n",
       "      <td>1.890625</td>\n",
       "      <td>1.855113</td>\n",
       "      <td>1.687260</td>\n",
       "      <td>1.882784</td>\n",
       "      <td>0.071141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.016098</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.201</td>\n",
       "      <td>{'alpha': 0.20099999999999996}</td>\n",
       "      <td>1.435151</td>\n",
       "      <td>1.596869</td>\n",
       "      <td>1.833134</td>\n",
       "      <td>1.885844</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923671</td>\n",
       "      <td>1.893094</td>\n",
       "      <td>1.932433</td>\n",
       "      <td>1.884166</td>\n",
       "      <td>1.875607</td>\n",
       "      <td>1.890622</td>\n",
       "      <td>1.855112</td>\n",
       "      <td>1.687256</td>\n",
       "      <td>1.882782</td>\n",
       "      <td>0.071141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.013572</td>\n",
       "      <td>0.010746</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.211</td>\n",
       "      <td>{'alpha': 0.21099999999999997}</td>\n",
       "      <td>1.435132</td>\n",
       "      <td>1.596890</td>\n",
       "      <td>1.833096</td>\n",
       "      <td>1.885821</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923671</td>\n",
       "      <td>1.893093</td>\n",
       "      <td>1.932428</td>\n",
       "      <td>1.884162</td>\n",
       "      <td>1.875604</td>\n",
       "      <td>1.890618</td>\n",
       "      <td>1.855112</td>\n",
       "      <td>1.687252</td>\n",
       "      <td>1.882779</td>\n",
       "      <td>0.071142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.013876</td>\n",
       "      <td>0.010736</td>\n",
       "      <td>0.006877</td>\n",
       "      <td>0.012480</td>\n",
       "      <td>0.221</td>\n",
       "      <td>{'alpha': 0.22099999999999997}</td>\n",
       "      <td>1.435113</td>\n",
       "      <td>1.596912</td>\n",
       "      <td>1.833057</td>\n",
       "      <td>1.885799</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923672</td>\n",
       "      <td>1.893092</td>\n",
       "      <td>1.932423</td>\n",
       "      <td>1.884158</td>\n",
       "      <td>1.875601</td>\n",
       "      <td>1.890614</td>\n",
       "      <td>1.855112</td>\n",
       "      <td>1.687248</td>\n",
       "      <td>1.882776</td>\n",
       "      <td>0.071142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.007951</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.231</td>\n",
       "      <td>{'alpha': 0.23099999999999996}</td>\n",
       "      <td>1.435094</td>\n",
       "      <td>1.596933</td>\n",
       "      <td>1.833018</td>\n",
       "      <td>1.885776</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923672</td>\n",
       "      <td>1.893091</td>\n",
       "      <td>1.932417</td>\n",
       "      <td>1.884155</td>\n",
       "      <td>1.875599</td>\n",
       "      <td>1.890610</td>\n",
       "      <td>1.855111</td>\n",
       "      <td>1.687244</td>\n",
       "      <td>1.882774</td>\n",
       "      <td>0.071142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.008406</td>\n",
       "      <td>0.004003</td>\n",
       "      <td>0.005826</td>\n",
       "      <td>0.241</td>\n",
       "      <td>{'alpha': 0.24099999999999996}</td>\n",
       "      <td>1.435076</td>\n",
       "      <td>1.596954</td>\n",
       "      <td>1.832979</td>\n",
       "      <td>1.885753</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923672</td>\n",
       "      <td>1.893090</td>\n",
       "      <td>1.932412</td>\n",
       "      <td>1.884151</td>\n",
       "      <td>1.875596</td>\n",
       "      <td>1.890606</td>\n",
       "      <td>1.855111</td>\n",
       "      <td>1.687240</td>\n",
       "      <td>1.882771</td>\n",
       "      <td>0.071143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.251</td>\n",
       "      <td>{'alpha': 0.25099999999999995}</td>\n",
       "      <td>1.435057</td>\n",
       "      <td>1.596975</td>\n",
       "      <td>1.832940</td>\n",
       "      <td>1.885730</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923673</td>\n",
       "      <td>1.893089</td>\n",
       "      <td>1.932407</td>\n",
       "      <td>1.884147</td>\n",
       "      <td>1.875593</td>\n",
       "      <td>1.890602</td>\n",
       "      <td>1.855111</td>\n",
       "      <td>1.687236</td>\n",
       "      <td>1.882769</td>\n",
       "      <td>0.071143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.008769</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>0.261</td>\n",
       "      <td>{'alpha': 0.26099999999999995}</td>\n",
       "      <td>1.435038</td>\n",
       "      <td>1.596996</td>\n",
       "      <td>1.832902</td>\n",
       "      <td>1.885707</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923673</td>\n",
       "      <td>1.893088</td>\n",
       "      <td>1.932402</td>\n",
       "      <td>1.884144</td>\n",
       "      <td>1.875590</td>\n",
       "      <td>1.890599</td>\n",
       "      <td>1.855110</td>\n",
       "      <td>1.687232</td>\n",
       "      <td>1.882766</td>\n",
       "      <td>0.071143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.271</td>\n",
       "      <td>{'alpha': 0.27099999999999996}</td>\n",
       "      <td>1.435019</td>\n",
       "      <td>1.597017</td>\n",
       "      <td>1.832863</td>\n",
       "      <td>1.885684</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923674</td>\n",
       "      <td>1.893088</td>\n",
       "      <td>1.932397</td>\n",
       "      <td>1.884140</td>\n",
       "      <td>1.875587</td>\n",
       "      <td>1.890595</td>\n",
       "      <td>1.855110</td>\n",
       "      <td>1.687228</td>\n",
       "      <td>1.882764</td>\n",
       "      <td>0.071143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007720</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.281</td>\n",
       "      <td>{'alpha': 0.28099999999999997}</td>\n",
       "      <td>1.435000</td>\n",
       "      <td>1.597038</td>\n",
       "      <td>1.832824</td>\n",
       "      <td>1.885662</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923674</td>\n",
       "      <td>1.893087</td>\n",
       "      <td>1.932392</td>\n",
       "      <td>1.884136</td>\n",
       "      <td>1.875584</td>\n",
       "      <td>1.890591</td>\n",
       "      <td>1.855110</td>\n",
       "      <td>1.687224</td>\n",
       "      <td>1.882761</td>\n",
       "      <td>0.071144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.006654</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>0.002943</td>\n",
       "      <td>0.291</td>\n",
       "      <td>{'alpha': 0.291}</td>\n",
       "      <td>1.434982</td>\n",
       "      <td>1.597059</td>\n",
       "      <td>1.832786</td>\n",
       "      <td>1.885639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923675</td>\n",
       "      <td>1.893086</td>\n",
       "      <td>1.932387</td>\n",
       "      <td>1.884133</td>\n",
       "      <td>1.875581</td>\n",
       "      <td>1.890587</td>\n",
       "      <td>1.855109</td>\n",
       "      <td>1.687220</td>\n",
       "      <td>1.882758</td>\n",
       "      <td>0.071144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.701</td>\n",
       "      <td>{'alpha': 0.7009999999999998}</td>\n",
       "      <td>1.434225</td>\n",
       "      <td>1.597911</td>\n",
       "      <td>1.831206</td>\n",
       "      <td>1.884706</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923691</td>\n",
       "      <td>1.893051</td>\n",
       "      <td>1.932177</td>\n",
       "      <td>1.883982</td>\n",
       "      <td>1.875465</td>\n",
       "      <td>1.890430</td>\n",
       "      <td>1.855094</td>\n",
       "      <td>1.687070</td>\n",
       "      <td>1.882653</td>\n",
       "      <td>0.071151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.711</td>\n",
       "      <td>{'alpha': 0.7109999999999999}</td>\n",
       "      <td>1.434207</td>\n",
       "      <td>1.597931</td>\n",
       "      <td>1.831168</td>\n",
       "      <td>1.884684</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923691</td>\n",
       "      <td>1.893051</td>\n",
       "      <td>1.932172</td>\n",
       "      <td>1.883978</td>\n",
       "      <td>1.875462</td>\n",
       "      <td>1.890426</td>\n",
       "      <td>1.855093</td>\n",
       "      <td>1.687067</td>\n",
       "      <td>1.882650</td>\n",
       "      <td>0.071151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.721</td>\n",
       "      <td>{'alpha': 0.7209999999999999}</td>\n",
       "      <td>1.434189</td>\n",
       "      <td>1.597952</td>\n",
       "      <td>1.831130</td>\n",
       "      <td>1.884661</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923691</td>\n",
       "      <td>1.893050</td>\n",
       "      <td>1.932167</td>\n",
       "      <td>1.883974</td>\n",
       "      <td>1.875459</td>\n",
       "      <td>1.890422</td>\n",
       "      <td>1.855093</td>\n",
       "      <td>1.687063</td>\n",
       "      <td>1.882648</td>\n",
       "      <td>0.071151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>0.009519</td>\n",
       "      <td>0.731</td>\n",
       "      <td>{'alpha': 0.7309999999999999}</td>\n",
       "      <td>1.434171</td>\n",
       "      <td>1.597972</td>\n",
       "      <td>1.831091</td>\n",
       "      <td>1.884638</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923692</td>\n",
       "      <td>1.893050</td>\n",
       "      <td>1.932162</td>\n",
       "      <td>1.883971</td>\n",
       "      <td>1.875456</td>\n",
       "      <td>1.890418</td>\n",
       "      <td>1.855092</td>\n",
       "      <td>1.687060</td>\n",
       "      <td>1.882645</td>\n",
       "      <td>0.071151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.005658</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.005465</td>\n",
       "      <td>0.741</td>\n",
       "      <td>{'alpha': 0.7409999999999999}</td>\n",
       "      <td>1.434153</td>\n",
       "      <td>1.597993</td>\n",
       "      <td>1.831053</td>\n",
       "      <td>1.884616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923692</td>\n",
       "      <td>1.893049</td>\n",
       "      <td>1.932157</td>\n",
       "      <td>1.883967</td>\n",
       "      <td>1.875453</td>\n",
       "      <td>1.890415</td>\n",
       "      <td>1.855092</td>\n",
       "      <td>1.687056</td>\n",
       "      <td>1.882643</td>\n",
       "      <td>0.071151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.007699</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.003210</td>\n",
       "      <td>0.751</td>\n",
       "      <td>{'alpha': 0.7509999999999999}</td>\n",
       "      <td>1.434136</td>\n",
       "      <td>1.598013</td>\n",
       "      <td>1.831015</td>\n",
       "      <td>1.884593</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923692</td>\n",
       "      <td>1.893048</td>\n",
       "      <td>1.932152</td>\n",
       "      <td>1.883963</td>\n",
       "      <td>1.875450</td>\n",
       "      <td>1.890411</td>\n",
       "      <td>1.855092</td>\n",
       "      <td>1.687053</td>\n",
       "      <td>1.882640</td>\n",
       "      <td>0.071151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.005456</td>\n",
       "      <td>0.761</td>\n",
       "      <td>{'alpha': 0.7609999999999999}</td>\n",
       "      <td>1.434120</td>\n",
       "      <td>1.598033</td>\n",
       "      <td>1.830977</td>\n",
       "      <td>1.884570</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923693</td>\n",
       "      <td>1.893048</td>\n",
       "      <td>1.932147</td>\n",
       "      <td>1.883960</td>\n",
       "      <td>1.875447</td>\n",
       "      <td>1.890407</td>\n",
       "      <td>1.855091</td>\n",
       "      <td>1.687050</td>\n",
       "      <td>1.882638</td>\n",
       "      <td>0.071151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.005946</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>0.002763</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.771</td>\n",
       "      <td>{'alpha': 0.7709999999999999}</td>\n",
       "      <td>1.434104</td>\n",
       "      <td>1.598054</td>\n",
       "      <td>1.830938</td>\n",
       "      <td>1.884548</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923693</td>\n",
       "      <td>1.893047</td>\n",
       "      <td>1.932142</td>\n",
       "      <td>1.883956</td>\n",
       "      <td>1.875445</td>\n",
       "      <td>1.890403</td>\n",
       "      <td>1.855091</td>\n",
       "      <td>1.687046</td>\n",
       "      <td>1.882635</td>\n",
       "      <td>0.071151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>0.781</td>\n",
       "      <td>{'alpha': 0.7809999999999999}</td>\n",
       "      <td>1.434088</td>\n",
       "      <td>1.598074</td>\n",
       "      <td>1.830900</td>\n",
       "      <td>1.884525</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923693</td>\n",
       "      <td>1.893047</td>\n",
       "      <td>1.932136</td>\n",
       "      <td>1.883952</td>\n",
       "      <td>1.875442</td>\n",
       "      <td>1.890399</td>\n",
       "      <td>1.855090</td>\n",
       "      <td>1.687043</td>\n",
       "      <td>1.882633</td>\n",
       "      <td>0.071152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.005461</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.791</td>\n",
       "      <td>{'alpha': 0.7909999999999999}</td>\n",
       "      <td>1.434072</td>\n",
       "      <td>1.598094</td>\n",
       "      <td>1.830862</td>\n",
       "      <td>1.884503</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923694</td>\n",
       "      <td>1.893046</td>\n",
       "      <td>1.932131</td>\n",
       "      <td>1.883949</td>\n",
       "      <td>1.875439</td>\n",
       "      <td>1.890396</td>\n",
       "      <td>1.855090</td>\n",
       "      <td>1.687039</td>\n",
       "      <td>1.882630</td>\n",
       "      <td>0.071152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.005931</td>\n",
       "      <td>0.004817</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.801</td>\n",
       "      <td>{'alpha': 0.8009999999999998}</td>\n",
       "      <td>1.434056</td>\n",
       "      <td>1.598115</td>\n",
       "      <td>1.830824</td>\n",
       "      <td>1.884480</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923694</td>\n",
       "      <td>1.893046</td>\n",
       "      <td>1.932126</td>\n",
       "      <td>1.883945</td>\n",
       "      <td>1.875436</td>\n",
       "      <td>1.890392</td>\n",
       "      <td>1.855089</td>\n",
       "      <td>1.687036</td>\n",
       "      <td>1.882628</td>\n",
       "      <td>0.071152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.811</td>\n",
       "      <td>{'alpha': 0.8109999999999998}</td>\n",
       "      <td>1.434040</td>\n",
       "      <td>1.598135</td>\n",
       "      <td>1.830785</td>\n",
       "      <td>1.884457</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923694</td>\n",
       "      <td>1.893045</td>\n",
       "      <td>1.932121</td>\n",
       "      <td>1.883941</td>\n",
       "      <td>1.875433</td>\n",
       "      <td>1.890388</td>\n",
       "      <td>1.855089</td>\n",
       "      <td>1.687032</td>\n",
       "      <td>1.882625</td>\n",
       "      <td>0.071152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.003485</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.821</td>\n",
       "      <td>{'alpha': 0.8209999999999998}</td>\n",
       "      <td>1.434024</td>\n",
       "      <td>1.598155</td>\n",
       "      <td>1.830747</td>\n",
       "      <td>1.884435</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923695</td>\n",
       "      <td>1.893045</td>\n",
       "      <td>1.932116</td>\n",
       "      <td>1.883937</td>\n",
       "      <td>1.875430</td>\n",
       "      <td>1.890384</td>\n",
       "      <td>1.855089</td>\n",
       "      <td>1.687029</td>\n",
       "      <td>1.882622</td>\n",
       "      <td>0.071152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.831</td>\n",
       "      <td>{'alpha': 0.8309999999999998}</td>\n",
       "      <td>1.434007</td>\n",
       "      <td>1.598175</td>\n",
       "      <td>1.830709</td>\n",
       "      <td>1.884412</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923695</td>\n",
       "      <td>1.893044</td>\n",
       "      <td>1.932111</td>\n",
       "      <td>1.883934</td>\n",
       "      <td>1.875427</td>\n",
       "      <td>1.890380</td>\n",
       "      <td>1.855088</td>\n",
       "      <td>1.687025</td>\n",
       "      <td>1.882620</td>\n",
       "      <td>0.071152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.004176</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.841</td>\n",
       "      <td>{'alpha': 0.8409999999999999}</td>\n",
       "      <td>1.433991</td>\n",
       "      <td>1.598196</td>\n",
       "      <td>1.830671</td>\n",
       "      <td>1.884389</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923695</td>\n",
       "      <td>1.893043</td>\n",
       "      <td>1.932106</td>\n",
       "      <td>1.883930</td>\n",
       "      <td>1.875425</td>\n",
       "      <td>1.890376</td>\n",
       "      <td>1.855088</td>\n",
       "      <td>1.687022</td>\n",
       "      <td>1.882617</td>\n",
       "      <td>0.071152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.851</td>\n",
       "      <td>{'alpha': 0.8509999999999999}</td>\n",
       "      <td>1.433975</td>\n",
       "      <td>1.598216</td>\n",
       "      <td>1.830632</td>\n",
       "      <td>1.884367</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923696</td>\n",
       "      <td>1.893043</td>\n",
       "      <td>1.932101</td>\n",
       "      <td>1.883926</td>\n",
       "      <td>1.875422</td>\n",
       "      <td>1.890373</td>\n",
       "      <td>1.855087</td>\n",
       "      <td>1.687018</td>\n",
       "      <td>1.882615</td>\n",
       "      <td>0.071152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.004795</td>\n",
       "      <td>0.861</td>\n",
       "      <td>{'alpha': 0.8609999999999999}</td>\n",
       "      <td>1.433959</td>\n",
       "      <td>1.598236</td>\n",
       "      <td>1.830594</td>\n",
       "      <td>1.884344</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923696</td>\n",
       "      <td>1.893042</td>\n",
       "      <td>1.932096</td>\n",
       "      <td>1.883923</td>\n",
       "      <td>1.875419</td>\n",
       "      <td>1.890369</td>\n",
       "      <td>1.855087</td>\n",
       "      <td>1.687015</td>\n",
       "      <td>1.882612</td>\n",
       "      <td>0.071153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.005485</td>\n",
       "      <td>0.002921</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.871</td>\n",
       "      <td>{'alpha': 0.8709999999999999}</td>\n",
       "      <td>1.433943</td>\n",
       "      <td>1.598256</td>\n",
       "      <td>1.830556</td>\n",
       "      <td>1.884321</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923696</td>\n",
       "      <td>1.893042</td>\n",
       "      <td>1.932092</td>\n",
       "      <td>1.883919</td>\n",
       "      <td>1.875416</td>\n",
       "      <td>1.890365</td>\n",
       "      <td>1.855086</td>\n",
       "      <td>1.687011</td>\n",
       "      <td>1.882610</td>\n",
       "      <td>0.071153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.008711</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.881</td>\n",
       "      <td>{'alpha': 0.8809999999999999}</td>\n",
       "      <td>1.433927</td>\n",
       "      <td>1.598277</td>\n",
       "      <td>1.830518</td>\n",
       "      <td>1.884299</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923697</td>\n",
       "      <td>1.893041</td>\n",
       "      <td>1.932088</td>\n",
       "      <td>1.883915</td>\n",
       "      <td>1.875413</td>\n",
       "      <td>1.890361</td>\n",
       "      <td>1.855086</td>\n",
       "      <td>1.687008</td>\n",
       "      <td>1.882607</td>\n",
       "      <td>0.071153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.006523</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.891</td>\n",
       "      <td>{'alpha': 0.8909999999999999}</td>\n",
       "      <td>1.433911</td>\n",
       "      <td>1.598297</td>\n",
       "      <td>1.830480</td>\n",
       "      <td>1.884276</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923697</td>\n",
       "      <td>1.893040</td>\n",
       "      <td>1.932083</td>\n",
       "      <td>1.883912</td>\n",
       "      <td>1.875410</td>\n",
       "      <td>1.890357</td>\n",
       "      <td>1.855085</td>\n",
       "      <td>1.687005</td>\n",
       "      <td>1.882605</td>\n",
       "      <td>0.071153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.005997</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.901</td>\n",
       "      <td>{'alpha': 0.9009999999999999}</td>\n",
       "      <td>1.433895</td>\n",
       "      <td>1.598317</td>\n",
       "      <td>1.830442</td>\n",
       "      <td>1.884254</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923697</td>\n",
       "      <td>1.893040</td>\n",
       "      <td>1.932079</td>\n",
       "      <td>1.883908</td>\n",
       "      <td>1.875407</td>\n",
       "      <td>1.890353</td>\n",
       "      <td>1.855085</td>\n",
       "      <td>1.687001</td>\n",
       "      <td>1.882602</td>\n",
       "      <td>0.071153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.006844</td>\n",
       "      <td>0.003055</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.911</td>\n",
       "      <td>{'alpha': 0.9109999999999998}</td>\n",
       "      <td>1.433879</td>\n",
       "      <td>1.598337</td>\n",
       "      <td>1.830404</td>\n",
       "      <td>1.884231</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923697</td>\n",
       "      <td>1.893039</td>\n",
       "      <td>1.932075</td>\n",
       "      <td>1.883904</td>\n",
       "      <td>1.875404</td>\n",
       "      <td>1.890350</td>\n",
       "      <td>1.855085</td>\n",
       "      <td>1.686998</td>\n",
       "      <td>1.882600</td>\n",
       "      <td>0.071153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>0.008352</td>\n",
       "      <td>0.921</td>\n",
       "      <td>{'alpha': 0.9209999999999998}</td>\n",
       "      <td>1.433863</td>\n",
       "      <td>1.598357</td>\n",
       "      <td>1.830365</td>\n",
       "      <td>1.884208</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923698</td>\n",
       "      <td>1.893039</td>\n",
       "      <td>1.932070</td>\n",
       "      <td>1.883900</td>\n",
       "      <td>1.875402</td>\n",
       "      <td>1.890346</td>\n",
       "      <td>1.855084</td>\n",
       "      <td>1.686994</td>\n",
       "      <td>1.882597</td>\n",
       "      <td>0.071154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.008870</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.006337</td>\n",
       "      <td>0.931</td>\n",
       "      <td>{'alpha': 0.9309999999999998}</td>\n",
       "      <td>1.433847</td>\n",
       "      <td>1.598377</td>\n",
       "      <td>1.830327</td>\n",
       "      <td>1.884186</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923698</td>\n",
       "      <td>1.893038</td>\n",
       "      <td>1.932066</td>\n",
       "      <td>1.883897</td>\n",
       "      <td>1.875399</td>\n",
       "      <td>1.890342</td>\n",
       "      <td>1.855084</td>\n",
       "      <td>1.686991</td>\n",
       "      <td>1.882595</td>\n",
       "      <td>0.071154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.007612</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.941</td>\n",
       "      <td>{'alpha': 0.9409999999999998}</td>\n",
       "      <td>1.433831</td>\n",
       "      <td>1.598397</td>\n",
       "      <td>1.830289</td>\n",
       "      <td>1.884163</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923698</td>\n",
       "      <td>1.893038</td>\n",
       "      <td>1.932062</td>\n",
       "      <td>1.883893</td>\n",
       "      <td>1.875396</td>\n",
       "      <td>1.890338</td>\n",
       "      <td>1.855083</td>\n",
       "      <td>1.686987</td>\n",
       "      <td>1.882593</td>\n",
       "      <td>0.071154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.006398</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.951</td>\n",
       "      <td>{'alpha': 0.9509999999999998}</td>\n",
       "      <td>1.433815</td>\n",
       "      <td>1.598417</td>\n",
       "      <td>1.830251</td>\n",
       "      <td>1.884141</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923699</td>\n",
       "      <td>1.893037</td>\n",
       "      <td>1.932058</td>\n",
       "      <td>1.883889</td>\n",
       "      <td>1.875393</td>\n",
       "      <td>1.890334</td>\n",
       "      <td>1.855083</td>\n",
       "      <td>1.686984</td>\n",
       "      <td>1.882590</td>\n",
       "      <td>0.071154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.961</td>\n",
       "      <td>{'alpha': 0.9609999999999999}</td>\n",
       "      <td>1.433799</td>\n",
       "      <td>1.598438</td>\n",
       "      <td>1.830213</td>\n",
       "      <td>1.884118</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923699</td>\n",
       "      <td>1.893036</td>\n",
       "      <td>1.932053</td>\n",
       "      <td>1.883886</td>\n",
       "      <td>1.875390</td>\n",
       "      <td>1.890331</td>\n",
       "      <td>1.855082</td>\n",
       "      <td>1.686980</td>\n",
       "      <td>1.882588</td>\n",
       "      <td>0.071154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.971</td>\n",
       "      <td>{'alpha': 0.9709999999999999}</td>\n",
       "      <td>1.433784</td>\n",
       "      <td>1.598458</td>\n",
       "      <td>1.830175</td>\n",
       "      <td>1.884095</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923699</td>\n",
       "      <td>1.893036</td>\n",
       "      <td>1.932049</td>\n",
       "      <td>1.883882</td>\n",
       "      <td>1.875387</td>\n",
       "      <td>1.890327</td>\n",
       "      <td>1.855082</td>\n",
       "      <td>1.686977</td>\n",
       "      <td>1.882585</td>\n",
       "      <td>0.071154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.005790</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.981</td>\n",
       "      <td>{'alpha': 0.9809999999999999}</td>\n",
       "      <td>1.433768</td>\n",
       "      <td>1.598478</td>\n",
       "      <td>1.830137</td>\n",
       "      <td>1.884073</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923700</td>\n",
       "      <td>1.893035</td>\n",
       "      <td>1.932045</td>\n",
       "      <td>1.883878</td>\n",
       "      <td>1.875384</td>\n",
       "      <td>1.890323</td>\n",
       "      <td>1.855081</td>\n",
       "      <td>1.686973</td>\n",
       "      <td>1.882583</td>\n",
       "      <td>0.071155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.005114</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.991</td>\n",
       "      <td>{'alpha': 0.9909999999999999}</td>\n",
       "      <td>1.433752</td>\n",
       "      <td>1.598498</td>\n",
       "      <td>1.830099</td>\n",
       "      <td>1.884050</td>\n",
       "      <td>...</td>\n",
       "      <td>1.923700</td>\n",
       "      <td>1.893035</td>\n",
       "      <td>1.932040</td>\n",
       "      <td>1.883875</td>\n",
       "      <td>1.875382</td>\n",
       "      <td>1.890319</td>\n",
       "      <td>1.855081</td>\n",
       "      <td>1.686970</td>\n",
       "      <td>1.882580</td>\n",
       "      <td>0.071155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0        0.004808      0.000708         0.001653        0.000337       0.001   \n",
       "1        0.008728      0.004812         0.002110        0.001199       0.011   \n",
       "2        0.009925      0.005437         0.001702        0.000286       0.021   \n",
       "3        0.009585      0.006721         0.002231        0.001593       0.031   \n",
       "4        0.006710      0.003723         0.001713        0.000313       0.041   \n",
       "5        0.006998      0.004127         0.002701        0.002286       0.051   \n",
       "6        0.009639      0.007851         0.001777        0.000544       0.061   \n",
       "7        0.007304      0.003465         0.001975        0.001084       0.071   \n",
       "8        0.008036      0.004077         0.001587        0.000197       0.081   \n",
       "9        0.004861      0.000381         0.003413        0.003330       0.091   \n",
       "10       0.006266      0.002574         0.003108        0.003925       0.101   \n",
       "11       0.006812      0.003585         0.002676        0.002457       0.111   \n",
       "12       0.007257      0.003730         0.001827        0.000491       0.121   \n",
       "13       0.005957      0.002304         0.001752        0.000303       0.131   \n",
       "14       0.007098      0.004970         0.002897        0.003527       0.141   \n",
       "15       0.019860      0.021024         0.001773        0.000413       0.151   \n",
       "16       0.014118      0.011156         0.002218        0.001008       0.161   \n",
       "17       0.017678      0.013558         0.004713        0.005965       0.171   \n",
       "18       0.012077      0.009928         0.006014        0.012117       0.181   \n",
       "19       0.014294      0.009394         0.002199        0.001622       0.191   \n",
       "20       0.016098      0.009509         0.003074        0.001706       0.201   \n",
       "21       0.013572      0.010746         0.001858        0.000424       0.211   \n",
       "22       0.013876      0.010736         0.006877        0.012480       0.221   \n",
       "23       0.007951      0.006600         0.001941        0.000569       0.231   \n",
       "24       0.010781      0.008406         0.004003        0.005826       0.241   \n",
       "25       0.009960      0.009347         0.002545        0.002582       0.251   \n",
       "26       0.008769      0.004764         0.003115        0.003838       0.261   \n",
       "27       0.006300      0.001973         0.002112        0.000968       0.271   \n",
       "28       0.007720      0.004096         0.001924        0.000802       0.281   \n",
       "29       0.009752      0.006654         0.003882        0.002943       0.291   \n",
       "..            ...           ...              ...             ...         ...   \n",
       "70       0.006338      0.003071         0.001737        0.000290       0.701   \n",
       "71       0.004401      0.000155         0.001484        0.000070       0.711   \n",
       "72       0.006430      0.002180         0.002704        0.003098       0.721   \n",
       "73       0.005959      0.002421         0.005553        0.009519       0.731   \n",
       "74       0.005658      0.002107         0.003932        0.005465       0.741   \n",
       "75       0.007699      0.004464         0.003127        0.003210       0.751   \n",
       "76       0.006989      0.003915         0.003308        0.005456       0.761   \n",
       "77       0.005946      0.004858         0.002763        0.003201       0.771   \n",
       "78       0.007335      0.003736         0.002490        0.003070       0.781   \n",
       "79       0.005461      0.003012         0.004546        0.005443       0.791   \n",
       "80       0.005931      0.004817         0.001454        0.000074       0.801   \n",
       "81       0.007194      0.005081         0.001854        0.001212       0.811   \n",
       "82       0.007529      0.003485         0.002120        0.001810       0.821   \n",
       "83       0.005450      0.003171         0.001883        0.000742       0.831   \n",
       "84       0.004176      0.000259         0.001445        0.000130       0.841   \n",
       "85       0.004329      0.000237         0.001541        0.000211       0.851   \n",
       "86       0.005700      0.001856         0.003925        0.004795       0.861   \n",
       "87       0.005485      0.002921         0.001568        0.000252       0.871   \n",
       "88       0.008711      0.007929         0.002146        0.001412       0.881   \n",
       "89       0.006523      0.002956         0.001545        0.000118       0.891   \n",
       "90       0.005997      0.002801         0.003533        0.003766       0.901   \n",
       "91       0.006844      0.003055         0.003085        0.003879       0.911   \n",
       "92       0.007527      0.003395         0.004696        0.008352       0.921   \n",
       "93       0.008870      0.006460         0.005824        0.006337       0.931   \n",
       "94       0.007612      0.003764         0.002566        0.001110       0.941   \n",
       "95       0.006398      0.001547         0.001929        0.000453       0.951   \n",
       "96       0.006500      0.001489         0.001918        0.000407       0.961   \n",
       "97       0.005168      0.000802         0.001960        0.000311       0.971   \n",
       "98       0.005790      0.000782         0.001739        0.000381       0.981   \n",
       "99       0.005114      0.001074         0.001602        0.000348       0.991   \n",
       "\n",
       "                             params  split0_test_score  split1_test_score  \\\n",
       "0                  {'alpha': 0.001}           1.435532           1.596441   \n",
       "1                  {'alpha': 0.011}           1.435513           1.596463   \n",
       "2   {'alpha': 0.020999999999999998}           1.435493           1.596485   \n",
       "3   {'alpha': 0.030999999999999996}           1.435474           1.596506   \n",
       "4   {'alpha': 0.040999999999999995}           1.435455           1.596527   \n",
       "5    {'alpha': 0.05099999999999999}           1.435436           1.596549   \n",
       "6    {'alpha': 0.06099999999999999}           1.435417           1.596570   \n",
       "7                  {'alpha': 0.071}           1.435398           1.596592   \n",
       "8    {'alpha': 0.08099999999999999}           1.435379           1.596613   \n",
       "9    {'alpha': 0.09099999999999998}           1.435360           1.596635   \n",
       "10   {'alpha': 0.10099999999999998}           1.435341           1.596656   \n",
       "11   {'alpha': 0.11099999999999999}           1.435322           1.596677   \n",
       "12   {'alpha': 0.12099999999999998}           1.435303           1.596699   \n",
       "13   {'alpha': 0.13099999999999998}           1.435284           1.596720   \n",
       "14                 {'alpha': 0.141}           1.435265           1.596741   \n",
       "15   {'alpha': 0.15099999999999997}           1.435246           1.596763   \n",
       "16   {'alpha': 0.16099999999999998}           1.435227           1.596784   \n",
       "17   {'alpha': 0.17099999999999999}           1.435208           1.596805   \n",
       "18   {'alpha': 0.18099999999999997}           1.435189           1.596827   \n",
       "19   {'alpha': 0.19099999999999998}           1.435170           1.596848   \n",
       "20   {'alpha': 0.20099999999999996}           1.435151           1.596869   \n",
       "21   {'alpha': 0.21099999999999997}           1.435132           1.596890   \n",
       "22   {'alpha': 0.22099999999999997}           1.435113           1.596912   \n",
       "23   {'alpha': 0.23099999999999996}           1.435094           1.596933   \n",
       "24   {'alpha': 0.24099999999999996}           1.435076           1.596954   \n",
       "25   {'alpha': 0.25099999999999995}           1.435057           1.596975   \n",
       "26   {'alpha': 0.26099999999999995}           1.435038           1.596996   \n",
       "27   {'alpha': 0.27099999999999996}           1.435019           1.597017   \n",
       "28   {'alpha': 0.28099999999999997}           1.435000           1.597038   \n",
       "29                 {'alpha': 0.291}           1.434982           1.597059   \n",
       "..                              ...                ...                ...   \n",
       "70    {'alpha': 0.7009999999999998}           1.434225           1.597911   \n",
       "71    {'alpha': 0.7109999999999999}           1.434207           1.597931   \n",
       "72    {'alpha': 0.7209999999999999}           1.434189           1.597952   \n",
       "73    {'alpha': 0.7309999999999999}           1.434171           1.597972   \n",
       "74    {'alpha': 0.7409999999999999}           1.434153           1.597993   \n",
       "75    {'alpha': 0.7509999999999999}           1.434136           1.598013   \n",
       "76    {'alpha': 0.7609999999999999}           1.434120           1.598033   \n",
       "77    {'alpha': 0.7709999999999999}           1.434104           1.598054   \n",
       "78    {'alpha': 0.7809999999999999}           1.434088           1.598074   \n",
       "79    {'alpha': 0.7909999999999999}           1.434072           1.598094   \n",
       "80    {'alpha': 0.8009999999999998}           1.434056           1.598115   \n",
       "81    {'alpha': 0.8109999999999998}           1.434040           1.598135   \n",
       "82    {'alpha': 0.8209999999999998}           1.434024           1.598155   \n",
       "83    {'alpha': 0.8309999999999998}           1.434007           1.598175   \n",
       "84    {'alpha': 0.8409999999999999}           1.433991           1.598196   \n",
       "85    {'alpha': 0.8509999999999999}           1.433975           1.598216   \n",
       "86    {'alpha': 0.8609999999999999}           1.433959           1.598236   \n",
       "87    {'alpha': 0.8709999999999999}           1.433943           1.598256   \n",
       "88    {'alpha': 0.8809999999999999}           1.433927           1.598277   \n",
       "89    {'alpha': 0.8909999999999999}           1.433911           1.598297   \n",
       "90    {'alpha': 0.9009999999999999}           1.433895           1.598317   \n",
       "91    {'alpha': 0.9109999999999998}           1.433879           1.598337   \n",
       "92    {'alpha': 0.9209999999999998}           1.433863           1.598357   \n",
       "93    {'alpha': 0.9309999999999998}           1.433847           1.598377   \n",
       "94    {'alpha': 0.9409999999999998}           1.433831           1.598397   \n",
       "95    {'alpha': 0.9509999999999998}           1.433815           1.598417   \n",
       "96    {'alpha': 0.9609999999999999}           1.433799           1.598438   \n",
       "97    {'alpha': 0.9709999999999999}           1.433784           1.598458   \n",
       "98    {'alpha': 0.9809999999999999}           1.433768           1.598478   \n",
       "99    {'alpha': 0.9909999999999999}           1.433752           1.598498   \n",
       "\n",
       "    split2_test_score  split3_test_score       ...         split2_train_score  \\\n",
       "0            1.833912           1.886301       ...                   1.923662   \n",
       "1            1.833873           1.886279       ...                   1.923662   \n",
       "2            1.833834           1.886256       ...                   1.923662   \n",
       "3            1.833795           1.886233       ...                   1.923663   \n",
       "4            1.833756           1.886210       ...                   1.923663   \n",
       "5            1.833717           1.886187       ...                   1.923664   \n",
       "6            1.833678           1.886164       ...                   1.923664   \n",
       "7            1.833639           1.886141       ...                   1.923665   \n",
       "8            1.833601           1.886118       ...                   1.923665   \n",
       "9            1.833562           1.886096       ...                   1.923666   \n",
       "10           1.833523           1.886073       ...                   1.923666   \n",
       "11           1.833484           1.886050       ...                   1.923667   \n",
       "12           1.833445           1.886027       ...                   1.923667   \n",
       "13           1.833406           1.886004       ...                   1.923668   \n",
       "14           1.833367           1.885981       ...                   1.923668   \n",
       "15           1.833328           1.885958       ...                   1.923668   \n",
       "16           1.833290           1.885936       ...                   1.923669   \n",
       "17           1.833251           1.885913       ...                   1.923669   \n",
       "18           1.833212           1.885890       ...                   1.923670   \n",
       "19           1.833173           1.885867       ...                   1.923670   \n",
       "20           1.833134           1.885844       ...                   1.923671   \n",
       "21           1.833096           1.885821       ...                   1.923671   \n",
       "22           1.833057           1.885799       ...                   1.923672   \n",
       "23           1.833018           1.885776       ...                   1.923672   \n",
       "24           1.832979           1.885753       ...                   1.923672   \n",
       "25           1.832940           1.885730       ...                   1.923673   \n",
       "26           1.832902           1.885707       ...                   1.923673   \n",
       "27           1.832863           1.885684       ...                   1.923674   \n",
       "28           1.832824           1.885662       ...                   1.923674   \n",
       "29           1.832786           1.885639       ...                   1.923675   \n",
       "..                ...                ...       ...                        ...   \n",
       "70           1.831206           1.884706       ...                   1.923691   \n",
       "71           1.831168           1.884684       ...                   1.923691   \n",
       "72           1.831130           1.884661       ...                   1.923691   \n",
       "73           1.831091           1.884638       ...                   1.923692   \n",
       "74           1.831053           1.884616       ...                   1.923692   \n",
       "75           1.831015           1.884593       ...                   1.923692   \n",
       "76           1.830977           1.884570       ...                   1.923693   \n",
       "77           1.830938           1.884548       ...                   1.923693   \n",
       "78           1.830900           1.884525       ...                   1.923693   \n",
       "79           1.830862           1.884503       ...                   1.923694   \n",
       "80           1.830824           1.884480       ...                   1.923694   \n",
       "81           1.830785           1.884457       ...                   1.923694   \n",
       "82           1.830747           1.884435       ...                   1.923695   \n",
       "83           1.830709           1.884412       ...                   1.923695   \n",
       "84           1.830671           1.884389       ...                   1.923695   \n",
       "85           1.830632           1.884367       ...                   1.923696   \n",
       "86           1.830594           1.884344       ...                   1.923696   \n",
       "87           1.830556           1.884321       ...                   1.923696   \n",
       "88           1.830518           1.884299       ...                   1.923697   \n",
       "89           1.830480           1.884276       ...                   1.923697   \n",
       "90           1.830442           1.884254       ...                   1.923697   \n",
       "91           1.830404           1.884231       ...                   1.923697   \n",
       "92           1.830365           1.884208       ...                   1.923698   \n",
       "93           1.830327           1.884186       ...                   1.923698   \n",
       "94           1.830289           1.884163       ...                   1.923698   \n",
       "95           1.830251           1.884141       ...                   1.923699   \n",
       "96           1.830213           1.884118       ...                   1.923699   \n",
       "97           1.830175           1.884095       ...                   1.923699   \n",
       "98           1.830137           1.884073       ...                   1.923700   \n",
       "99           1.830099           1.884050       ...                   1.923700   \n",
       "\n",
       "    split3_train_score  split4_train_score  split5_train_score  \\\n",
       "0             1.893111            1.932538            1.884239   \n",
       "1             1.893110            1.932532            1.884235   \n",
       "2             1.893109            1.932527            1.884232   \n",
       "3             1.893108            1.932521            1.884228   \n",
       "4             1.893107            1.932516            1.884224   \n",
       "5             1.893106            1.932511            1.884221   \n",
       "6             1.893106            1.932505            1.884217   \n",
       "7             1.893105            1.932500            1.884213   \n",
       "8             1.893104            1.932495            1.884210   \n",
       "9             1.893103            1.932489            1.884206   \n",
       "10            1.893102            1.932484            1.884202   \n",
       "11            1.893101            1.932479            1.884199   \n",
       "12            1.893100            1.932474            1.884195   \n",
       "13            1.893100            1.932468            1.884191   \n",
       "14            1.893099            1.932463            1.884188   \n",
       "15            1.893098            1.932458            1.884184   \n",
       "16            1.893097            1.932453            1.884180   \n",
       "17            1.893096            1.932448            1.884177   \n",
       "18            1.893095            1.932443            1.884173   \n",
       "19            1.893094            1.932438            1.884169   \n",
       "20            1.893094            1.932433            1.884166   \n",
       "21            1.893093            1.932428            1.884162   \n",
       "22            1.893092            1.932423            1.884158   \n",
       "23            1.893091            1.932417            1.884155   \n",
       "24            1.893090            1.932412            1.884151   \n",
       "25            1.893089            1.932407            1.884147   \n",
       "26            1.893088            1.932402            1.884144   \n",
       "27            1.893088            1.932397            1.884140   \n",
       "28            1.893087            1.932392            1.884136   \n",
       "29            1.893086            1.932387            1.884133   \n",
       "..                 ...                 ...                 ...   \n",
       "70            1.893051            1.932177            1.883982   \n",
       "71            1.893051            1.932172            1.883978   \n",
       "72            1.893050            1.932167            1.883974   \n",
       "73            1.893050            1.932162            1.883971   \n",
       "74            1.893049            1.932157            1.883967   \n",
       "75            1.893048            1.932152            1.883963   \n",
       "76            1.893048            1.932147            1.883960   \n",
       "77            1.893047            1.932142            1.883956   \n",
       "78            1.893047            1.932136            1.883952   \n",
       "79            1.893046            1.932131            1.883949   \n",
       "80            1.893046            1.932126            1.883945   \n",
       "81            1.893045            1.932121            1.883941   \n",
       "82            1.893045            1.932116            1.883937   \n",
       "83            1.893044            1.932111            1.883934   \n",
       "84            1.893043            1.932106            1.883930   \n",
       "85            1.893043            1.932101            1.883926   \n",
       "86            1.893042            1.932096            1.883923   \n",
       "87            1.893042            1.932092            1.883919   \n",
       "88            1.893041            1.932088            1.883915   \n",
       "89            1.893040            1.932083            1.883912   \n",
       "90            1.893040            1.932079            1.883908   \n",
       "91            1.893039            1.932075            1.883904   \n",
       "92            1.893039            1.932070            1.883900   \n",
       "93            1.893038            1.932066            1.883897   \n",
       "94            1.893038            1.932062            1.883893   \n",
       "95            1.893037            1.932058            1.883889   \n",
       "96            1.893036            1.932053            1.883886   \n",
       "97            1.893036            1.932049            1.883882   \n",
       "98            1.893035            1.932045            1.883878   \n",
       "99            1.893035            1.932040            1.883875   \n",
       "\n",
       "    split6_train_score  split7_train_score  split8_train_score  \\\n",
       "0             1.875665            1.890698            1.855118   \n",
       "1             1.875662            1.890694            1.855118   \n",
       "2             1.875660            1.890691            1.855118   \n",
       "3             1.875657            1.890687            1.855117   \n",
       "4             1.875654            1.890683            1.855117   \n",
       "5             1.875651            1.890679            1.855117   \n",
       "6             1.875648            1.890675            1.855117   \n",
       "7             1.875645            1.890671            1.855116   \n",
       "8             1.875642            1.890668            1.855116   \n",
       "9             1.875639            1.890664            1.855116   \n",
       "10            1.875636            1.890660            1.855115   \n",
       "11            1.875633            1.890656            1.855115   \n",
       "12            1.875631            1.890652            1.855115   \n",
       "13            1.875628            1.890648            1.855114   \n",
       "14            1.875625            1.890645            1.855114   \n",
       "15            1.875622            1.890641            1.855114   \n",
       "16            1.875619            1.890637            1.855113   \n",
       "17            1.875616            1.890633            1.855113   \n",
       "18            1.875613            1.890629            1.855113   \n",
       "19            1.875610            1.890625            1.855113   \n",
       "20            1.875607            1.890622            1.855112   \n",
       "21            1.875604            1.890618            1.855112   \n",
       "22            1.875601            1.890614            1.855112   \n",
       "23            1.875599            1.890610            1.855111   \n",
       "24            1.875596            1.890606            1.855111   \n",
       "25            1.875593            1.890602            1.855111   \n",
       "26            1.875590            1.890599            1.855110   \n",
       "27            1.875587            1.890595            1.855110   \n",
       "28            1.875584            1.890591            1.855110   \n",
       "29            1.875581            1.890587            1.855109   \n",
       "..                 ...                 ...                 ...   \n",
       "70            1.875465            1.890430            1.855094   \n",
       "71            1.875462            1.890426            1.855093   \n",
       "72            1.875459            1.890422            1.855093   \n",
       "73            1.875456            1.890418            1.855092   \n",
       "74            1.875453            1.890415            1.855092   \n",
       "75            1.875450            1.890411            1.855092   \n",
       "76            1.875447            1.890407            1.855091   \n",
       "77            1.875445            1.890403            1.855091   \n",
       "78            1.875442            1.890399            1.855090   \n",
       "79            1.875439            1.890396            1.855090   \n",
       "80            1.875436            1.890392            1.855089   \n",
       "81            1.875433            1.890388            1.855089   \n",
       "82            1.875430            1.890384            1.855089   \n",
       "83            1.875427            1.890380            1.855088   \n",
       "84            1.875425            1.890376            1.855088   \n",
       "85            1.875422            1.890373            1.855087   \n",
       "86            1.875419            1.890369            1.855087   \n",
       "87            1.875416            1.890365            1.855086   \n",
       "88            1.875413            1.890361            1.855086   \n",
       "89            1.875410            1.890357            1.855085   \n",
       "90            1.875407            1.890353            1.855085   \n",
       "91            1.875404            1.890350            1.855085   \n",
       "92            1.875402            1.890346            1.855084   \n",
       "93            1.875399            1.890342            1.855084   \n",
       "94            1.875396            1.890338            1.855083   \n",
       "95            1.875393            1.890334            1.855083   \n",
       "96            1.875390            1.890331            1.855082   \n",
       "97            1.875387            1.890327            1.855082   \n",
       "98            1.875384            1.890323            1.855081   \n",
       "99            1.875382            1.890319            1.855081   \n",
       "\n",
       "    split9_train_score  mean_train_score  std_train_score  \n",
       "0             1.687341          1.882834         0.071135  \n",
       "1             1.687337          1.882832         0.071135  \n",
       "2             1.687333          1.882829         0.071135  \n",
       "3             1.687329          1.882826         0.071136  \n",
       "4             1.687324          1.882824         0.071136  \n",
       "5             1.687320          1.882821         0.071136  \n",
       "6             1.687316          1.882818         0.071137  \n",
       "7             1.687311          1.882816         0.071137  \n",
       "8             1.687307          1.882813         0.071137  \n",
       "9             1.687303          1.882811         0.071138  \n",
       "10            1.687299          1.882808         0.071138  \n",
       "11            1.687294          1.882805         0.071138  \n",
       "12            1.687290          1.882803         0.071139  \n",
       "13            1.687286          1.882800         0.071139  \n",
       "14            1.687282          1.882797         0.071139  \n",
       "15            1.687277          1.882795         0.071140  \n",
       "16            1.687273          1.882792         0.071140  \n",
       "17            1.687269          1.882790         0.071140  \n",
       "18            1.687265          1.882787         0.071141  \n",
       "19            1.687260          1.882784         0.071141  \n",
       "20            1.687256          1.882782         0.071141  \n",
       "21            1.687252          1.882779         0.071142  \n",
       "22            1.687248          1.882776         0.071142  \n",
       "23            1.687244          1.882774         0.071142  \n",
       "24            1.687240          1.882771         0.071143  \n",
       "25            1.687236          1.882769         0.071143  \n",
       "26            1.687232          1.882766         0.071143  \n",
       "27            1.687228          1.882764         0.071143  \n",
       "28            1.687224          1.882761         0.071144  \n",
       "29            1.687220          1.882758         0.071144  \n",
       "..                 ...               ...              ...  \n",
       "70            1.687070          1.882653         0.071151  \n",
       "71            1.687067          1.882650         0.071151  \n",
       "72            1.687063          1.882648         0.071151  \n",
       "73            1.687060          1.882645         0.071151  \n",
       "74            1.687056          1.882643         0.071151  \n",
       "75            1.687053          1.882640         0.071151  \n",
       "76            1.687050          1.882638         0.071151  \n",
       "77            1.687046          1.882635         0.071151  \n",
       "78            1.687043          1.882633         0.071152  \n",
       "79            1.687039          1.882630         0.071152  \n",
       "80            1.687036          1.882628         0.071152  \n",
       "81            1.687032          1.882625         0.071152  \n",
       "82            1.687029          1.882622         0.071152  \n",
       "83            1.687025          1.882620         0.071152  \n",
       "84            1.687022          1.882617         0.071152  \n",
       "85            1.687018          1.882615         0.071152  \n",
       "86            1.687015          1.882612         0.071153  \n",
       "87            1.687011          1.882610         0.071153  \n",
       "88            1.687008          1.882607         0.071153  \n",
       "89            1.687005          1.882605         0.071153  \n",
       "90            1.687001          1.882602         0.071153  \n",
       "91            1.686998          1.882600         0.071153  \n",
       "92            1.686994          1.882597         0.071154  \n",
       "93            1.686991          1.882595         0.071154  \n",
       "94            1.686987          1.882593         0.071154  \n",
       "95            1.686984          1.882590         0.071154  \n",
       "96            1.686980          1.882588         0.071154  \n",
       "97            1.686977          1.882585         0.071154  \n",
       "98            1.686973          1.882583         0.071155  \n",
       "99            1.686970          1.882580         0.071155  \n",
       "\n",
       "[100 rows x 31 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gr.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=123890, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(gr.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -2.067863\n",
       "1    -2.067664\n",
       "2    -2.067465\n",
       "3    -2.067268\n",
       "4    -2.067073\n",
       "5    -2.066879\n",
       "6    -2.066687\n",
       "7    -2.066496\n",
       "8    -2.066307\n",
       "9    -2.066119\n",
       "10   -2.065931\n",
       "11   -2.065746\n",
       "12   -2.065561\n",
       "13   -2.065377\n",
       "14   -2.065195\n",
       "15   -2.065014\n",
       "16   -2.064833\n",
       "17   -2.064654\n",
       "18   -2.064476\n",
       "19   -2.064299\n",
       "Name: mean_test_score, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['mean_test_score'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [lass\n",
    "elasticnet\n",
    "boosting\n",
    "randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {model.__class__.__name__:0 for model in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df, columns=['Medu', 'Fedu', 'traveltime', 'studytime', 'school', 'sex',\\\n",
    "                            'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', \\\n",
    "                            'guardian', 'schoolsup', 'famsup', 'paid', 'activities', \\\n",
    "                            'nursery', 'higher', 'internet', 'romantic'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
